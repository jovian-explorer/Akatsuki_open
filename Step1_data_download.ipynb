{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Download level 2 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All data downloaded and converted to CSV successfully for the vcors_1001 directory.\n",
      "All data downloaded and converted to CSV successfully for the vcors_1002 directory.\n",
      "All data downloaded and converted to CSV successfully for the vcors_1003 directory.\n",
      "All data downloaded and converted to CSV successfully for the vcors_1004 directory.\n",
      "All data downloaded, converted to CSV, and .tab files deleted successfully.\n"
     ]
    }
   ],
   "source": [
    "# Define the base URL and directory path\n",
    "base_url = 'https://data.darts.isas.jaxa.jp/pub/pds3/vco-v-rs-3-occ-v1.0/'\n",
    "base_directory = '/home/dev/Desktop/Venus/Data/'\n",
    "\n",
    "# Mapping of old directory names to new directory names\n",
    "directory_mapping = {\n",
    "    'vcors_1001': 'vcors_2001',\n",
    "    'vcors_1002': 'vcors_2002',\n",
    "    'vcors_1003': 'vcors_2003',\n",
    "    'vcors_1004': 'vcors_2004'\n",
    "}\n",
    "\n",
    "# Create the base directory if it doesn't exist\n",
    "if not os.path.exists(base_directory):\n",
    "    os.makedirs(base_directory)\n",
    "\n",
    "# Function to download a file from URL\n",
    "def download_file(url, file_path):\n",
    "    response = requests.get(url)\n",
    "    with open(file_path, 'wb') as file:\n",
    "        file.write(response.content)\n",
    "\n",
    "# Function to convert tab file to CSV\n",
    "def convert_to_csv(args):\n",
    "    directory, subdir, tab_file = args\n",
    "    url = base_url + directory + '/data/' + subdir + '/'\n",
    "    file_path = os.path.join(base_directory, directory_mapping[directory], subdir, tab_file)\n",
    "    download_file(url + tab_file, file_path)\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, delim_whitespace=True, header=None)\n",
    "        csv_file = os.path.join(base_directory, directory_mapping[directory], subdir, os.path.splitext(tab_file)[0] + '.csv')\n",
    "        df.to_csv(csv_file, index=False)\n",
    "        os.remove(file_path)  # Delete the .tab file\n",
    "        return csv_file\n",
    "    except pd.errors.ParserError:\n",
    "        print(f\"Error processing file: {file_path}. Skipping...\")\n",
    "        os.remove(file_path)  # Delete the problematic file\n",
    "        return None\n",
    "\n",
    "# List of directories to process\n",
    "directories = ['vcors_1001', 'vcors_1002', 'vcors_1003', 'vcors_1004']\n",
    "subdirectories = ['l2']\n",
    "\n",
    "# Create a pool of worker processes\n",
    "pool = Pool()\n",
    "\n",
    "# Convert tab files to CSV using multiprocessing\n",
    "for directory in directories:\n",
    "    directory_path = os.path.join(base_directory, directory_mapping[directory])\n",
    "    \n",
    "    # Create the directory if it doesn't exist\n",
    "    if not os.path.exists(directory_path):\n",
    "        os.makedirs(directory_path)\n",
    "    \n",
    "    for subdir in subdirectories:\n",
    "        subdir_path = os.path.join(directory_path, subdir)\n",
    "        \n",
    "        # Create the subdirectory if it doesn't exist\n",
    "        if not os.path.exists(subdir_path):\n",
    "            os.makedirs(subdir_path)\n",
    "        \n",
    "        # Define the URL and get the list of tab files\n",
    "        url = base_url + directory + '/data/' + subdir + '/'\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        tab_files = [a['href'] for a in soup.find_all('a') if a['href'].endswith('.tab')]\n",
    "        \n",
    "        # Perform parallel conversion for each tab file\n",
    "        args = zip([directory] * len(tab_files), [subdir] * len(tab_files), tab_files)\n",
    "        csv_files = pool.map(convert_to_csv, args)\n",
    "        csv_files = [f for f in csv_files if f is not None]  # Remove None values\n",
    "    print(f\"All data downloaded and converted to CSV successfully for the {directory} directory.\")\n",
    "\n",
    "# Close the pool to prevent any more tasks from being submitted\n",
    "pool.close()\n",
    "\n",
    "# Wait for all processes in the pool to finish\n",
    "pool.join()\n",
    "\n",
    "print(\"All data downloaded, converted to CSV, and .tab files deleted successfully.\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Download level 3 and level 4 data, and sort it accordingly as per \"ingress\" or \"egress\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data downloaded, converted to CSV, and sorted successfully for the l3 directory.\n",
      "Data downloaded, converted to CSV, and sorted successfully for the l4 directory.\n",
      "All data downloaded, converted to CSV, and sorted successfully for the vcors_2001 directory.\n",
      "Data downloaded, converted to CSV, and sorted successfully for the l3 directory.\n",
      "Data downloaded, converted to CSV, and sorted successfully for the l4 directory.\n",
      "All data downloaded, converted to CSV, and sorted successfully for the vcors_2002 directory.\n",
      "Data downloaded, converted to CSV, and sorted successfully for the l3 directory.\n",
      "Data downloaded, converted to CSV, and sorted successfully for the l4 directory.\n",
      "All data downloaded, converted to CSV, and sorted successfully for the vcors_2003 directory.\n",
      "Data downloaded, converted to CSV, and sorted successfully for the l3 directory.\n",
      "Data downloaded, converted to CSV, and sorted successfully for the l4 directory.\n",
      "All data downloaded, converted to CSV, and sorted successfully for the vcors_2004 directory.\n",
      "All data downloaded, converted to CSV, and sorted successfully.\n"
     ]
    }
   ],
   "source": [
    "# Define the base URL and directory path\n",
    "base_url = 'https://data.darts.isas.jaxa.jp/pub/pds3/vco-v-rs-5-occ-v1.0/'\n",
    "base_directory = '/home/dev/Desktop/Venus/Data/'\n",
    "\n",
    "# Create the base directory if it doesn't exist\n",
    "if not os.path.exists(base_directory):\n",
    "    os.makedirs(base_directory)\n",
    "\n",
    "# Function to download a file from URL\n",
    "def download_file(url, file_path):\n",
    "    response = requests.get(url)\n",
    "    with open(file_path, 'wb') as file:\n",
    "        file.write(response.content)\n",
    "\n",
    "# Function to convert tab file to CSV\n",
    "def convert_to_csv(args):\n",
    "    directory, subdir, tab_file = args\n",
    "    url = base_url + directory + '/data/' + subdir + '/'\n",
    "    file_path = os.path.join(base_directory, directory, subdir, tab_file)\n",
    "    download_file(url + tab_file, file_path)\n",
    "    df = pd.read_csv(file_path, delim_whitespace=True, header=None)\n",
    "    csv_file = os.path.join(base_directory, directory, subdir, os.path.splitext(tab_file)[0] + '.csv')\n",
    "    df.to_csv(csv_file, index=False)\n",
    "    os.remove(file_path)  # Delete the .tab file\n",
    "    return csv_file\n",
    "\n",
    "# List of directories to process\n",
    "directories = ['vcors_2001', 'vcors_2002', 'vcors_2003', 'vcors_2004']\n",
    "subdirectories = ['l3', 'l4']\n",
    "\n",
    "# Create a pool of worker processes\n",
    "pool = Pool()\n",
    "\n",
    "# Convert tab files to CSV using multiprocessing\n",
    "for directory in directories:\n",
    "    directory_path = os.path.join(base_directory, directory)\n",
    "\n",
    "    # Create the directory if it doesn't exist\n",
    "    if not os.path.exists(directory_path):\n",
    "        os.makedirs(directory_path)\n",
    "\n",
    "    for subdir in subdirectories:\n",
    "        subdir_path = os.path.join(directory_path, subdir)\n",
    "\n",
    "        # Create the subdirectory if it doesn't exist\n",
    "        if not os.path.exists(subdir_path):\n",
    "            os.makedirs(subdir_path)\n",
    "\n",
    "        # Define the URL and get the list of tab files\n",
    "        url = base_url + directory + '/data/' + subdir + '/'\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        tab_files = [a['href'] for a in soup.find_all('a') if a['href'].endswith('.tab')]\n",
    "\n",
    "        # Perform parallel conversion for each tab file\n",
    "        args = zip([directory] * len(tab_files), [subdir] * len(tab_files), tab_files)\n",
    "        csv_files = pool.map(convert_to_csv, args)\n",
    "\n",
    "        # Create subdirectories for \"ingress\" and \"egress\"\n",
    "        ingress_subdir = os.path.join(directory_path, subdir, 'ingress')\n",
    "        egress_subdir = os.path.join(directory_path, subdir, 'egress')\n",
    "        os.makedirs(ingress_subdir, exist_ok=True)\n",
    "        os.makedirs(egress_subdir, exist_ok=True)\n",
    "\n",
    "        for csv_file in csv_files:\n",
    "            file_name = os.path.basename(csv_file)\n",
    "\n",
    "            # Check if the filename contains 'i' or 'e'\n",
    "            if 'i' in file_name.lower():\n",
    "                destination = os.path.join(ingress_subdir, file_name)\n",
    "            elif 'e' in file_name.lower():\n",
    "                destination = os.path.join(egress_subdir, file_name)\n",
    "            else:\n",
    "                destination = os.path.join(directory_path, subdir, file_name)\n",
    "\n",
    "            # Move the CSV file to the appropriate directory\n",
    "            os.rename(csv_file, destination)\n",
    "        print(f\"Data downloaded, converted to CSV, and sorted successfully for the {subdir} directory.\")\n",
    "\n",
    "    print(f\"All data downloaded, converted to CSV, and sorted successfully for the {directory} directory.\")\n",
    "\n",
    "# Close the pool to prevent any more tasks from being submitted\n",
    "pool.close()\n",
    "\n",
    "# Wait for all processes in the pool to finish\n",
    "pool.join()\n",
    "\n",
    "print(\"All data downloaded, converted to CSV, and sorted successfully.\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of files downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CSV files with 'l2' in their name: 69\n",
      "Number of CSV files with 'l3' in their name: 111\n",
      "Number of CSV files with 'l4' in their name: 112\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "path = '/home/dev/Desktop/Venus/Data'\n",
    "directories = ['vcors_2001', 'vcors_2002', 'vcors_2003', 'vcors_2004']\n",
    "subdirectories = ['l2', 'l3', 'l4']\n",
    "subsubdirectories = ['ingress', 'egress']\n",
    "\n",
    "l2_count = 0\n",
    "l3_count = 0\n",
    "l4_count = 0\n",
    "\n",
    "for directory in directories:\n",
    "    for subdirectory in subdirectories:\n",
    "        if subdirectory == 'l3' or subdirectory == 'l4':\n",
    "            for subsubdirectory in subsubdirectories:\n",
    "                folder_path = os.path.join(path, directory, subdirectory, subsubdirectory)\n",
    "                csv_files = glob.glob(os.path.join(folder_path, '*.csv'))\n",
    "                for csv_file in csv_files:\n",
    "                    filename = os.path.basename(csv_file)\n",
    "                    if 'l2' in filename.lower():\n",
    "                        l2_count += 1\n",
    "                    if 'l3' in filename.lower():\n",
    "                        l3_count += 1\n",
    "                    if 'l4' in filename.lower():\n",
    "                        l4_count += 1\n",
    "        else:\n",
    "            folder_path = os.path.join(path, directory, subdirectory)\n",
    "            csv_files = glob.glob(os.path.join(folder_path, '*.csv'))\n",
    "            for csv_file in csv_files:\n",
    "                filename = os.path.basename(csv_file)\n",
    "                if 'l2' in filename.lower():\n",
    "                    l2_count += 1\n",
    "                if 'l3' in filename.lower():\n",
    "                    l3_count += 1\n",
    "                if 'l4' in filename.lower():\n",
    "                    l4_count += 1\n",
    "\n",
    "print(f\"Number of CSV files with 'l2' in their name: {l2_count}\")\n",
    "print(f\"Number of CSV files with 'l3' in their name: {l3_count}\")\n",
    "print(f\"Number of CSV files with 'l4' in their name: {l4_count}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
