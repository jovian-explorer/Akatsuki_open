{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afa7bb8b",
   "metadata": {},
   "source": [
    "## AKATSUKI Radio Science (RS) Data Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7e2755",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2587825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify url for l2, l3 and l4 RS data\n",
    "data_url_l2 = 'https://data.darts.isas.jaxa.jp/pub/pds3/vco-v-rs-3-occ-v1.0/'\n",
    "data_url_l3l4 = 'https://data.darts.isas.jaxa.jp/pub/pds3/vco-v-rs-5-occ-v1.0/'\n",
    "\n",
    "data_url_list = [data_url_l2 , data_url_l3l4]\n",
    "\n",
    "# specify local directory to store downloaded data\n",
    "data_directory = '/home/hb/Desktop/Akatsuki/Data/'\n",
    "\n",
    "# specify local directory to store final merged .csv files data\n",
    "working_data_directory = '/home/hb/Desktop/Akatsuki/working_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e7c616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create specified directory path if it doesn't exist\n",
    "if not os.path.exists(data_directory):\n",
    "    os.makedirs(data_directory)\n",
    "    \n",
    "if not os.path.exists(working_data_directory):\n",
    "    os.makedirs(working_data_directory)\n",
    "    \n",
    "if not os.path.exists(working_data_directory + '/merged_ig_data'):\n",
    "    os.makedirs(working_data_directory + '/merged_ig_data')\n",
    "\n",
    "if not os.path.exists(working_data_directory + '/merged_eg_data'):\n",
    "    os.makedirs(working_data_directory + '/merged_eg_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a346fdea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download all files ending in .zip from the specified url\n",
    "for data_url in data_url_list:\n",
    "    response = requests.get(data_url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    zip_files_list = [a['href'] for a in soup.find_all('a') if a['href'].endswith('.zip')]\n",
    "\n",
    "    for zip_file in zip_files_list:\n",
    "        dwnld_url = data_url + zip_file\n",
    "        response = requests.get(dwnld_url)\n",
    "        data_file = data_directory + zip_file\n",
    "        with open(data_file , 'wb') as file:\n",
    "            file.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8635b212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unzip all downloaded files and delete original .zip files\n",
    "dwnld_zip_files = os.listdir(data_directory)\n",
    "for zip_file in dwnld_zip_files:\n",
    "    zipfilepath = data_directory + zip_file\n",
    "    shutil.unpack_archive(zipfilepath,data_directory)\n",
    "    os.remove(zipfilepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b0baee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting relevant .tab files for conversion to .csv files\n",
    "\n",
    "data_subdir_list = []\n",
    "\n",
    "for root, dirs, files in os.walk(data_directory):\n",
    "    for name in dirs:\n",
    "        if 'data' in name:\n",
    "            filtered_dirs = os.path.join(root, name)\n",
    "            data_subdir_list.append(filtered_dirs)\n",
    "\n",
    "filtered_l2tabfile_list =[]\n",
    "filtered_l3tabfile_list =[]\n",
    "filtered_l4tabfile_list =[]\n",
    "\n",
    "for subdir in data_subdir_list:\n",
    "    for root, dirs, files in os.walk(subdir):\n",
    "        for name in files:\n",
    "            if '.tab' in name and 'l2' in name:\n",
    "                filtered_files = os.path.join(root, name)\n",
    "                filtered_l2tabfile_list.append(filtered_files)                \n",
    "            elif '.tab' in name and 'l3' in name:\n",
    "                filtered_files = os.path.join(root, name)\n",
    "                filtered_l3tabfile_list.append(filtered_files)\n",
    "            elif '.tab' in name and 'l4' in name:\n",
    "                filtered_files = os.path.join(root, name)\n",
    "                filtered_l4tabfile_list.append(filtered_files)\n",
    "\n",
    "print(len(filtered_l2tabfile_list))\n",
    "print(len(filtered_l3tabfile_list))\n",
    "print(len(filtered_l4tabfile_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc37cfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert .tab files to .csv files with proper headers\n",
    "\n",
    "l2_columns = [\"SAMPLE_NUMBER\",\n",
    "              \"UTC_TIME\",\n",
    "              \"DAY_OF_YEAR_WITH_FRACTIONS\",\n",
    "              \"EPHEMERIS_SECONDS\",\n",
    "              \"DISTANCE\",\n",
    "              \"TRANSMIT_FREQUENCY_RAMP_REFERENCE_TIME\",\n",
    "              \"TRANSMIT_FREQUENCY_CONSTANT_TERM\",\n",
    "              \"TRANSMIT_FREQUENCY_LINEAR_TERM\",\n",
    "              \"OBSERVED_X_BAND_ANTENNA_FREQUENCY\",\n",
    "              \"PREDICTED_X_BAND_ANTENNA_FREQUENCY\",\n",
    "              \"CORRECTION_OF_EARTH_ATMOSPHERE_PROPAGATION\",\n",
    "              \"RESIDUAL_CALIBRATED_X_BAND_FREQUENCY_SHIFT\",\n",
    "              \"SIGNAL_LEVEL_X_BAND\",\n",
    "              \"DIFFERENTIAL_DOPPLER\",\n",
    "              \"SIGMA_OBSERVED_X_BAND_ANTENNA_FREQUENCY\",\n",
    "              \"SIGNAL_QUALITY_X_BAND\",\n",
    "              \"SIGMA_SIGNAL_LEVEL_X_BAND\"]\n",
    "\n",
    "l3_columns = [\"SAMPLE_NUMBER\",\n",
    "              \"UTC_TIME\",\n",
    "              \"EPHEMERIS_SECONDS\",\n",
    "              \"RESIDUAL_CALIBRATED_X_BAND_FREQUENCY_SHIFT\",\n",
    "              \"RESIDUAL_CALIBRATED_X_BAND_FREQUENCY_SHIFT_AFTER_BASELINE_FIT\",\n",
    "              \"RECONSTRUCTED_TRANSMIT_FREQUENCY\",\n",
    "              \"RADIUS\",\n",
    "              \"SIGMA_RADIUS\",\n",
    "              \"BENDING_ANGLE\",\n",
    "              \"SIGMA_BENDING_ANGLE\",\n",
    "              \"REFRACTIVE_INDEX\",\n",
    "              \"REFRACTIVITY\",\n",
    "              \"SIGMA_REFRACTIVITY\",\n",
    "              \"SIGNAL_LEVEL\",\n",
    "              \"DIFFERENTIAL_DOPPLER\",\n",
    "              \"IMPACT_PARAMETER\",\n",
    "              \"SIGMA_IMPACT_PARAMETER\",\n",
    "              \"LONGITUDE\",\n",
    "              \"LATITUDE\"]\n",
    "\n",
    "l4_columns = [\"SAMPLE_NUMBER\",\n",
    "              \"UTC_TIME\",\n",
    "              \"EPHEMERIS_SECONDS\",\n",
    "              \"RADIUS\",\n",
    "              \"LATITUDE\",\n",
    "              \"LONGITUDE\",\n",
    "              \"GEOPOTENTIAL\",\n",
    "              \"GEOPOTENTIAL_HEIGHT\",\n",
    "              \"PRESSURE_LOWER_TEMPERATURE_AT_BOUNDARY\",\n",
    "              \"SIGMA_PRESSURE_LOWER_TEMPERATURE_AT_BOUNDARY\",\n",
    "              \"PRESSURE_MEDIUM_TEMPERATURE_AT_BOUNDARY\",\n",
    "              \"SIGMA_PRESSURE_MEDIUM_TEMPERATURE_AT_BOUNDARY\",\n",
    "              \"PRESSURE_HIGHER_TEMPERATURE_AT_BOUNDARY\",\n",
    "              \"SIGMA_PRESSURE_HIGHER_TEMPERATURE_AT_BOUNDARY\",\n",
    "              \"TEMPERATURE_LOWER_TEMPERATURE_AT_BOUNDARY\",\n",
    "              \"SIGMA_TEMPERATURE_LOWER_TEMPERATURE_AT_BOUNDARY\",\n",
    "              \"TEMPERATURE_MEDIUM_TEMPERATURE_AT_BOUNDARY\",\n",
    "              \"SIGMA_TEMPERATURE_MEDIUM_TEMPERATURE_AT_BOUNDARY\",\n",
    "              \"TEMPERATURE_HIGHER_TEMPERATURE_AT_BOUNDARY\",\n",
    "              \"SIGMA_TEMPERATURE_HIGHER_TEMPERATURE_AT_BOUNDARY\",\n",
    "              \"NUMBER_DENSITY\",\n",
    "              \"SIGMA_NUMBER_DENSITY\",\n",
    "              \"SOLAR_ZENITH_ANGLE\",\n",
    "              \"LOCAL_SOLAR_TIME\"]\n",
    "\n",
    "common_columns = []\n",
    "for element in l2_columns:\n",
    "    if element in l3_columns :\n",
    "        common_columns.append(element)\n",
    "print(common_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd733b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all converted l2, l3, l4 .csv files to be stored in directory 'working_data_directory'\n",
    "\n",
    "# converting l2 .tab files to .csv files\n",
    "for tabfile in filtered_l2tabfile_list:\n",
    "    df = pd.read_csv(tabfile, delim_whitespace=True, header=None)\n",
    "    csv_file = os.path.join(working_data_directory, os.path.splitext(os.path.basename(tabfile))[0] + '.csv')\n",
    "    df.to_csv(csv_file, header=l2_columns, index=False)\n",
    "\n",
    "# converting l3 .tab files to .csv files\n",
    "for tabfile in filtered_l3tabfile_list:\n",
    "    df = pd.read_csv(tabfile, delim_whitespace=True, header=None)\n",
    "    csv_file = os.path.join(working_data_directory, os.path.splitext(os.path.basename(tabfile))[0] + '.csv')\n",
    "    df.to_csv(csv_file, header=l3_columns, index=False)\n",
    "\n",
    "# converting l4 .tab files to .csv files\n",
    "for tabfile in filtered_l4tabfile_list:\n",
    "    df = pd.read_csv(tabfile, delim_whitespace=True, header=None)\n",
    "    csv_file = os.path.join(working_data_directory, os.path.splitext(os.path.basename(tabfile))[0] + '.csv')\n",
    "    df.to_csv(csv_file, header=l4_columns, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1018a133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting instances when l2 data was recorded\n",
    "\n",
    "data_instances = []\n",
    "\n",
    "for filename in filtered_l2tabfile_list:\n",
    "    group_name = os.path.basename(filename)[0:18]\n",
    "    data_instances.append(group_name)\n",
    "\n",
    "data_instances.sort()\n",
    "print(len(data_instances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcad38c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouping together similar instances of l2, l3 and l4 data\n",
    "# ideally there should be 5 files in each data-instance group\n",
    "\n",
    "data_group = []\n",
    "\n",
    "for data_instance in data_instances:\n",
    "    data_instance_group = []\n",
    "    for root, dirs, files in os.walk(working_data_directory):           \n",
    "        for name in files: \n",
    "            if data_instance in name:\n",
    "                data_instance_group.append(name)\n",
    "                data_instance_group.sort()\n",
    "    data_group.append(data_instance_group)\n",
    "print(len(data_group))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b6bf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "j=0\n",
    "for i in data_group:\n",
    "    if len(i) == 5:\n",
    "        j=j+1\n",
    "print('no. of data_instance_groups with 5 files: ',j, '\\n')\n",
    "\n",
    "j=0\n",
    "for i in data_group:\n",
    "    if len(i) != 5:\n",
    "        j=j+1\n",
    "print('no. of data_instance_groups with more or less than 5 files',j,'\\n\\n')\n",
    "        \n",
    "for i in data_group:\n",
    "    if len(i) != 5:\n",
    "        print(len(i),':', i, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc88ca90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort each data-instance group\n",
    "\n",
    "for data_instance_group in data_group:\n",
    "    data_instance_group.sort()\n",
    "\n",
    "print(data_group[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5690f3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# dataframe merging\n",
    "\n",
    "for data_instance_group in  data_group:\n",
    "    if len(data_instance_group) >= 3:\n",
    "        ingress_dataframes = []\n",
    "        egress_dataframes = []\n",
    "        for csv_file in data_instance_group:\n",
    "            if '_ai_v10' in csv_file or '_i_v10' in csv_file:\n",
    "                df = pd.read_csv(os.path.join(working_data_directory,csv_file))\n",
    "                ingress_dataframes.append(df.drop(['SAMPLE_NUMBER'], axis=1))\n",
    "            if '_ae_v10' in csv_file or '_e_v10' in csv_file:\n",
    "                df = pd.read_csv(os.path.join(working_data_directory,csv_file))\n",
    "                egress_dataframes.append(df.drop(['SAMPLE_NUMBER'], axis=1))\n",
    "            if 'l2' in csv_file:\n",
    "                l2_df = pd.read_csv(os.path.join(working_data_directory,csv_file))\n",
    "                l2_df = l2_df.drop(['SAMPLE_NUMBER'], axis=1)\n",
    "        \n",
    "        if (len(ingress_dataframes)) == 2:\n",
    "            merged_ingress_dataframe = pd.merge(ingress_dataframes[0], ingress_dataframes[1], how='outer', sort=True, on = ['UTC_TIME','EPHEMERIS_SECONDS'], suffixes=(None, '_duplicate')) \n",
    "        final_merged_ingress_dataframe = pd.merge(merged_ingress_dataframe, l2_df, how='left', on = ['UTC_TIME','EPHEMERIS_SECONDS'])\n",
    "        final_merged_ingress_csv = os.path.join(working_data_directory + '/merged_ig_data', os.path.basename(data_instance_group[0])[0:24] + '_ing.csv')\n",
    "        final_merged_ingress_dataframe.to_csv(final_merged_ingress_csv, index=False)\n",
    "        \n",
    "            \n",
    "        if (len(egress_dataframes)) == 2:\n",
    "            merged_egress_dataframe = pd.merge(egress_dataframes[0], egress_dataframes[1], how='outer', sort=True, on = ['UTC_TIME','EPHEMERIS_SECONDS'], suffixes=(None, '_duplicate')) \n",
    "        final_merged_egress_dataframe = pd.merge(merged_egress_dataframe, l2_df, how='left', on = ['UTC_TIME','EPHEMERIS_SECONDS'])\n",
    "        final_merged_egress_csv = os.path.join(working_data_directory + '/merged_eg_data', os.path.basename(data_instance_group[0])[0:24] + '_eg.csv')\n",
    "        final_merged_egress_dataframe.to_csv(final_merged_egress_csv, index=False)\n",
    "        \n",
    "        #display(merged_ingress_dataframe)\n",
    "        #display(final_merged_ingress_dataframe)\n",
    "        #display(merged_egress_dataframe)\n",
    "        #display(final_merged_egress_dataframe)\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
